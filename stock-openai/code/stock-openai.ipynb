{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents loaded successfully!\n",
      "Press Release  \n",
      "New\tDelhi,\tAugust\t10,\t2023  \n",
      " \n",
      "\t\n",
      "HERO\tMOTOCORP \tREPORTS \tREVENUE \tOF\tRs.\t8,767\tCRORE\tIN\t\n",
      "Q1\tFY’24\t\n",
      "\t\n",
      "DELIVERS \tPROFIT\tGROWTH \tOF\t51%\t\n",
      "\t\n",
      "Highlights \tfor\tQ1\tFY’24\t(April‐June\t2023)\n",
      " \n",
      " Volume  – 13.53 lakh units of motorcycles and scooters sold in Q1 FY’2 4 \n",
      " \n",
      " Total\tIncome  – Rs. 8,989 Crore, a growth of 6% over the corresponding quart er \n",
      "in the previous quarter   \n",
      " \tEBITDA  for Q1 FY’24 stands at Rs. 1,206 Crore; growth of 28 % \n",
      " \n",
      " Profit\tBefore\tTax\t(PBT)\tand\texceptional \tite\n",
      "r_splitter initialized successfully!\n",
      "docs split successfully!\n",
      "embedding initialized successfully!\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for text-embedding-ada-002 in organization org-Dkm0pVeh6YJTsFg1uATxQSFg on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\61086818\\OneDrive - LTIMindtree\\Desktop\\ppk-dev\\stock-openai\\code\\stock-openai.ipynb Cell 1\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/61086818/OneDrive%20-%20LTIMindtree/Desktop/ppk-dev/stock-openai/code/stock-openai.ipynb#W1sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvectorstores\u001b[39;00m \u001b[39mimport\u001b[39;00m Chroma\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/61086818/OneDrive%20-%20LTIMindtree/Desktop/ppk-dev/stock-openai/code/stock-openai.ipynb#W1sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m persist_directory \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../VectorDB\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/61086818/OneDrive%20-%20LTIMindtree/Desktop/ppk-dev/stock-openai/code/stock-openai.ipynb#W1sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m vectordb \u001b[39m=\u001b[39m Chroma\u001b[39m.\u001b[39;49mfrom_documents(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/61086818/OneDrive%20-%20LTIMindtree/Desktop/ppk-dev/stock-openai/code/stock-openai.ipynb#W1sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     documents\u001b[39m=\u001b[39;49msplits,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/61086818/OneDrive%20-%20LTIMindtree/Desktop/ppk-dev/stock-openai/code/stock-openai.ipynb#W1sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     embedding\u001b[39m=\u001b[39;49membedding,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/61086818/OneDrive%20-%20LTIMindtree/Desktop/ppk-dev/stock-openai/code/stock-openai.ipynb#W1sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     persist_directory\u001b[39m=\u001b[39;49mpersist_directory\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/61086818/OneDrive%20-%20LTIMindtree/Desktop/ppk-dev/stock-openai/code/stock-openai.ipynb#W1sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/61086818/OneDrive%20-%20LTIMindtree/Desktop/ppk-dev/stock-openai/code/stock-openai.ipynb#W1sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mvectordb initialized successfully!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/61086818/OneDrive%20-%20LTIMindtree/Desktop/ppk-dev/stock-openai/code/stock-openai.ipynb#W1sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m vectordb \u001b[39m=\u001b[39m Chroma(persist_directory\u001b[39m=\u001b[39mpersist_directory, embedding_function\u001b[39m=\u001b[39membedding)\n",
      "File \u001b[1;32mc:\\Users\\61086818\\miniconda3\\lib\\site-packages\\langchain\\vectorstores\\chroma.py:771\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[1;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    769\u001b[0m texts \u001b[39m=\u001b[39m [doc\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents]\n\u001b[0;32m    770\u001b[0m metadatas \u001b[39m=\u001b[39m [doc\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m--> 771\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfrom_texts(\n\u001b[0;32m    772\u001b[0m     texts\u001b[39m=\u001b[39mtexts,\n\u001b[0;32m    773\u001b[0m     embedding\u001b[39m=\u001b[39membedding,\n\u001b[0;32m    774\u001b[0m     metadatas\u001b[39m=\u001b[39mmetadatas,\n\u001b[0;32m    775\u001b[0m     ids\u001b[39m=\u001b[39mids,\n\u001b[0;32m    776\u001b[0m     collection_name\u001b[39m=\u001b[39mcollection_name,\n\u001b[0;32m    777\u001b[0m     persist_directory\u001b[39m=\u001b[39mpersist_directory,\n\u001b[0;32m    778\u001b[0m     client_settings\u001b[39m=\u001b[39mclient_settings,\n\u001b[0;32m    779\u001b[0m     client\u001b[39m=\u001b[39mclient,\n\u001b[0;32m    780\u001b[0m     collection_metadata\u001b[39m=\u001b[39mcollection_metadata,\n\u001b[0;32m    781\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    782\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\61086818\\miniconda3\\lib\\site-packages\\langchain\\vectorstores\\chroma.py:729\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    721\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mchromadb\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbatch_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m create_batches\n\u001b[0;32m    723\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m create_batches(\n\u001b[0;32m    724\u001b[0m         api\u001b[39m=\u001b[39mchroma_collection\u001b[39m.\u001b[39m_client,\n\u001b[0;32m    725\u001b[0m         ids\u001b[39m=\u001b[39mids,\n\u001b[0;32m    726\u001b[0m         metadatas\u001b[39m=\u001b[39mmetadatas,\n\u001b[0;32m    727\u001b[0m         documents\u001b[39m=\u001b[39mtexts,\n\u001b[0;32m    728\u001b[0m     ):\n\u001b[1;32m--> 729\u001b[0m         chroma_collection\u001b[39m.\u001b[39;49madd_texts(\n\u001b[0;32m    730\u001b[0m             texts\u001b[39m=\u001b[39;49mbatch[\u001b[39m3\u001b[39;49m] \u001b[39mif\u001b[39;49;00m batch[\u001b[39m3\u001b[39;49m] \u001b[39melse\u001b[39;49;00m [],\n\u001b[0;32m    731\u001b[0m             metadatas\u001b[39m=\u001b[39;49mbatch[\u001b[39m2\u001b[39;49m] \u001b[39mif\u001b[39;49;00m batch[\u001b[39m2\u001b[39;49m] \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    732\u001b[0m             ids\u001b[39m=\u001b[39;49mbatch[\u001b[39m0\u001b[39;49m],\n\u001b[0;32m    733\u001b[0m         )\n\u001b[0;32m    734\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    735\u001b[0m     chroma_collection\u001b[39m.\u001b[39madd_texts(texts\u001b[39m=\u001b[39mtexts, metadatas\u001b[39m=\u001b[39mmetadatas, ids\u001b[39m=\u001b[39mids)\n",
      "File \u001b[1;32mc:\\Users\\61086818\\miniconda3\\lib\\site-packages\\langchain\\vectorstores\\chroma.py:275\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m texts \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(texts)\n\u001b[0;32m    274\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 275\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embedding_function\u001b[39m.\u001b[39;49membed_documents(texts)\n\u001b[0;32m    276\u001b[0m \u001b[39mif\u001b[39;00m metadatas:\n\u001b[0;32m    277\u001b[0m     \u001b[39m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[0;32m    278\u001b[0m     \u001b[39m# did not specify metadata for all texts\u001b[39;00m\n\u001b[0;32m    279\u001b[0m     length_diff \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(texts) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(metadatas)\n",
      "File \u001b[1;32mc:\\Users\\61086818\\miniconda3\\lib\\site-packages\\langchain\\embeddings\\openai.py:556\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[39m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \u001b[39m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[0;32m    555\u001b[0m engine \u001b[39m=\u001b[39m cast(\u001b[39mstr\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeployment)\n\u001b[1;32m--> 556\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_len_safe_embeddings(texts, engine\u001b[39m=\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\61086818\\miniconda3\\lib\\site-packages\\langchain\\embeddings\\openai.py:432\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    429\u001b[0m     _iter \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(tokens), _chunk_size)\n\u001b[0;32m    431\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m _iter:\n\u001b[1;32m--> 432\u001b[0m     response \u001b[39m=\u001b[39m embed_with_retry(\n\u001b[0;32m    433\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    434\u001b[0m         \u001b[39minput\u001b[39m\u001b[39m=\u001b[39mtokens[i : i \u001b[39m+\u001b[39m _chunk_size],\n\u001b[0;32m    435\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_invocation_params,\n\u001b[0;32m    436\u001b[0m     )\n\u001b[0;32m    437\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    438\u001b[0m         response \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mdict()\n",
      "File \u001b[1;32mc:\\Users\\61086818\\miniconda3\\lib\\site-packages\\langchain\\embeddings\\openai.py:107\u001b[0m, in \u001b[0;36membed_with_retry\u001b[1;34m(embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Use tenacity to retry the embedding call.\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[39mif\u001b[39;00m _is_openai_v1():\n\u001b[1;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m embeddings\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    108\u001b[0m retry_decorator \u001b[39m=\u001b[39m _create_retry_decorator(embeddings)\n\u001b[0;32m    110\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m    111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_embed_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n",
      "File \u001b[1;32mc:\\Users\\61086818\\miniconda3\\lib\\site-packages\\openai\\resources\\embeddings.py:105\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[1;34m(self, input, model, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m     99\u001b[0m         embedding\u001b[39m.\u001b[39membedding \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfrombuffer(  \u001b[39m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[0;32m    100\u001b[0m             base64\u001b[39m.\u001b[39mb64decode(data), dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m         )\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m    103\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\n\u001b[1;32m--> 105\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[0;32m    106\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m/embeddings\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    107\u001b[0m     body\u001b[39m=\u001b[39;49mmaybe_transform(params, embedding_create_params\u001b[39m.\u001b[39;49mEmbeddingCreateParams),\n\u001b[0;32m    108\u001b[0m     options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[0;32m    109\u001b[0m         extra_headers\u001b[39m=\u001b[39;49mextra_headers,\n\u001b[0;32m    110\u001b[0m         extra_query\u001b[39m=\u001b[39;49mextra_query,\n\u001b[0;32m    111\u001b[0m         extra_body\u001b[39m=\u001b[39;49mextra_body,\n\u001b[0;32m    112\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    113\u001b[0m         post_parser\u001b[39m=\u001b[39;49mparser,\n\u001b[0;32m    114\u001b[0m     ),\n\u001b[0;32m    115\u001b[0m     cast_to\u001b[39m=\u001b[39;49mCreateEmbeddingResponse,\n\u001b[0;32m    116\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\61086818\\miniconda3\\lib\\site-packages\\openai\\_base_client.py:1096\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1082\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[0;32m   1083\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1084\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1091\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1092\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m   1093\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[0;32m   1094\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[0;32m   1095\u001b[0m     )\n\u001b[1;32m-> 1096\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[1;32mc:\\Users\\61086818\\miniconda3\\lib\\site-packages\\openai\\_base_client.py:856\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    847\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    848\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    849\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    854\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    855\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m--> 856\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[0;32m    857\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[0;32m    858\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[0;32m    859\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    860\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m    861\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[0;32m    862\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\61086818\\miniconda3\\lib\\site-packages\\openai\\_base_client.py:894\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[0;32m    893\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[1;32m--> 894\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[0;32m    895\u001b[0m         options,\n\u001b[0;32m    896\u001b[0m         cast_to,\n\u001b[0;32m    897\u001b[0m         retries,\n\u001b[0;32m    898\u001b[0m         err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    899\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    900\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m    901\u001b[0m     )\n\u001b[0;32m    903\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    904\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    905\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\61086818\\miniconda3\\lib\\site-packages\\openai\\_base_client.py:966\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    963\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    964\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 966\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[0;32m    967\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[0;32m    968\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[0;32m    969\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[0;32m    970\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    971\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m    972\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\61086818\\miniconda3\\lib\\site-packages\\openai\\_base_client.py:894\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[0;32m    893\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[1;32m--> 894\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[0;32m    895\u001b[0m         options,\n\u001b[0;32m    896\u001b[0m         cast_to,\n\u001b[0;32m    897\u001b[0m         retries,\n\u001b[0;32m    898\u001b[0m         err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    899\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    900\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m    901\u001b[0m     )\n\u001b[0;32m    903\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    904\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    905\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\61086818\\miniconda3\\lib\\site-packages\\openai\\_base_client.py:966\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    963\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    964\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 966\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[0;32m    967\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[0;32m    968\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[0;32m    969\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[0;32m    970\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    971\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m    972\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\61086818\\miniconda3\\lib\\site-packages\\openai\\_base_client.py:908\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    905\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n\u001b[0;32m    906\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[1;32m--> 908\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    909\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    910\u001b[0m     \u001b[39mif\u001b[39;00m response \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for text-embedding-ada-002 in organization org-Dkm0pVeh6YJTsFg1uATxQSFg on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "\n",
    "sys.path.append('../..')\n",
    "openai.api_key = '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'\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "loaders = [\n",
    "    #PyPDFLoader(\"../Documents/GFL_Q1_FY24.pdf\"),\n",
    "    #PyPDFLoader(\"../Documents/HeroMoto_Q1_FY24.pdf\")\n",
    "\n",
    "    PyPDFLoader(\"../Documents/hero-motocorp-q1fy24_press-release.pdf\"),\n",
    "    PyPDFLoader(\"../Documents/tata-motors-group-results-databank-q1fy20.pdf\")\n",
    "    \n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "print(\"Documents loaded successfully!\")\n",
    "\n",
    "len(docs)\n",
    "\n",
    "doc = docs[0]\n",
    "print(doc.page_content[0:500])\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 100\n",
    "\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap = chunk_overlap,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")\n",
    "print(\"r_splitter initialized successfully!\")\n",
    "\n",
    "splits = r_splitter.split_documents(docs)\n",
    "print(\"docs split successfully!\")\n",
    "\n",
    "len(splits)\n",
    "\n",
    "splits[2]\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "openai_api_type = \"azure\"\n",
    "openai_api_ver = \"2022-12-01\"\n",
    "openai_api_key = openai.api_key\n",
    "embeddings_deployment_name = \"ppk-embed\"\n",
    "deployment_name = \"ppk-deploy\"\n",
    "\n",
    "embedding = OpenAIEmbeddings(deployment=embeddings_deployment_name,\n",
    "                            openai_api_key=openai_api_key,\n",
    "                            chunk_size=1,\n",
    "                            openai_api_version=openai_api_ver,\n",
    "                            openai_api_type=openai_api_type)\n",
    "\n",
    "print(\"embedding initialized successfully!\")\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = \"../VectorDB\"\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "print(\"vectordb initialized successfully!\")\n",
    "\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "\n",
    "import json\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(deployment_name=deployment_name, openai_api_version=openai_api_ver, openai_api_key=openai_api_key, temperature=0)\n",
    "\n",
    "# Help method to extract contents from the file\n",
    "\n",
    "# instruction - Should contain question to be asked\n",
    "# insight - Should include what insight is expected from the result- e.g. \"Headwind\", \"Tailwind\", etc.\n",
    "# bool_return_json - Should include Tru or False\n",
    "    # If True, method returns result in JSON format, else returns as dictionary. Default is True\n",
    "\n",
    "def extract_contents_without_prompt_engg(instruction, insight, bool_return_json=True):\n",
    "    dict_all_results = {}\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever = vectordb.as_retriever(search_kwargs={'k':3}),\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    result = qa_chain({'query':instruction})\n",
    "    dict_all_results[insight] = result['result'].split('\\n')\n",
    "    \n",
    "    if bool_return_json:\n",
    "        json_all_results = json.dumps(dict_all_results)\n",
    "        return json_all_results\n",
    "    else:\n",
    "        return dict_all_results\n",
    "\n",
    "instruction = \"What are the headwinds for Gujarat Fluorochemicals?\"\n",
    "insight = \"Headwinds\"\n",
    "\n",
    "json_insight = extract_contents_without_prompt_engg(instruction, insight, True)\n",
    "\n",
    "print(json_insight) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
